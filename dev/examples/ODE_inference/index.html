<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimization-Based ODE Inference Examples · DiffEqParamEstim.jl</title><link rel="canonical" href="https://diffeqparamestim.sciml.ai/stable/examples/ODE_inference/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DiffEqParamEstim.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">DiffEqParamEstim.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">DiffEqParamEstim.jl: Parameter Estimation for Differential Equations</a></li><li><span class="tocitem">Methods</span><ul><li><a class="tocitem" href="../../methods/recommended_methods/">Recommended Methods</a></li><li><a class="tocitem" href="../../methods/optimization_based_methods/">Optimization-Based Methods</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Optimization-Based ODE Inference Examples</a><ul class="internal"><li><a class="tocitem" href="#Simple-Local-Optimization"><span>Simple Local Optimization</span></a></li><li><a class="tocitem" href="#More-Algorithms-(Global-Optimization)-via-MathProgBase-Solvers"><span>More Algorithms (Global Optimization) via MathProgBase Solvers</span></a></li><li><a class="tocitem" href="#Using-JuMP-with-DiffEqParamEstim"><span>Using JuMP with DiffEqParamEstim</span></a></li><li><a class="tocitem" href="#Generalized-Likelihood-Example"><span>Generalized Likelihood Example</span></a></li></ul></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/stochastic_evaluations/">Parameter Estimation for Stochastic Differential Equations and Ensembles</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Optimization-Based ODE Inference Examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimization-Based ODE Inference Examples</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqParamEstim.jl/blob/master/docs/src/examples/ODE_inference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Optimization-Based-ODE-Inference-Examples"><a class="docs-heading-anchor" href="#Optimization-Based-ODE-Inference-Examples">Optimization-Based ODE Inference Examples</a><a id="Optimization-Based-ODE-Inference-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-Based-ODE-Inference-Examples" title="Permalink"></a></h1><h2 id="Simple-Local-Optimization"><a class="docs-heading-anchor" href="#Simple-Local-Optimization">Simple Local Optimization</a><a id="Simple-Local-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Simple-Local-Optimization" title="Permalink"></a></h2><p>We choose to optimize the parameters on the Lotka-Volterra equation. We do so by defining the function as a function with parameters:</p><pre><code class="language-julia">function f(du,u,p,t)
  du[1] = dx = p[1]*u[1] - u[1]*u[2]
  du[2] = dy = -3*u[2] + u[1]*u[2]
end

u0 = [1.0;1.0]
tspan = (0.0,10.0)
p = [1.5]
prob = ODEProblem(f,u0,tspan,p)</code></pre><p>We create data using the numerical result with <code>a=1.5</code>:</p><pre><code class="language-julia">sol = solve(prob,Tsit5())
t = collect(range(0,stop=10,length=200))
using RecursiveArrayTools # for VectorOfArray
randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])
data = convert(Array,randomized)</code></pre><p>Here we used <code>VectorOfArray</code> from <a href="https://github.com/ChrisRackauckas/RecursiveArrayTools.jl">RecursiveArrayTools.jl</a> to turn the result of an ODE into a matrix.</p><p>If we plot the solution with the parameter at <code>a=1.42</code>, we get the following:</p><p><img src="assets/paramest_notfit.png" alt="Parameter Estimation Not Fit"/></p><p>Notice that after one period this solution begins to drift very far off: this problem is sensitive to the choice of <code>a</code>.</p><p>To build the objective function for Optim.jl, we simply call the <code>build_loss_objective</code> function:</p><pre><code class="language-julia">cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data),
                                     maxiters=10000,verbose=false)</code></pre><p>This objective function internally is calling the ODE solver to get solutions to test against the data. The keyword arguments are passed directly to the solver. Note that we set <code>maxiters</code> in a way that causes the differential equation solvers to error more quickly when in bad regions of the parameter space, speeding up the process. If the integrator stops early (due to divergence), then those parameters are given an infinite loss, and thus this is a quick way to avoid bad parameters. We set <code>verbose=false</code> because this divergence can get noisy.</p><p>Before optimizing, let&#39;s visualize our cost function by plotting it for a range of parameter values:</p><pre><code class="language-julia">vals = 0.0:0.1:10.0
using Plots; plotly()
plot(vals,[cost_function(i) for i in vals],yscale=:log10,
     xaxis = &quot;Parameter&quot;, yaxis = &quot;Cost&quot;, title = &quot;1-Parameter Cost Function&quot;,
     lw = 3)</code></pre><p><img src="assets/1paramcost.png" alt="1 Parameter Likelihood"/></p><p>Here we see that there is a very well-defined minimum in our cost function at the real parameter (because this is where the solution almost exactly fits the dataset).</p><p>Now this cost function can be used with Optim.jl in order to get the parameters. For example, we can use Brent&#39;s algorithm to search for the best solution on the interval <code>[0,10]</code> by:</p><pre><code class="language-julia">using Optim
result = optimize(cost_function, 0.0, 10.0)</code></pre><p>This returns <code>result.minimizer[1]==1.5</code> as the best parameter to match the data. When we plot the fitted equation on the data, we receive the following:</p><p><img src="assets/paramest_fit.png" alt="Parameter Estimation Fit"/></p><p>Thus we see that after fitting, the lines match up with the generated data and receive the right parameter value.</p><p>We can also use the multivariate optimization functions. For example, we can use the <code>BFGS</code> algorithm to optimize the parameter starting at <code>a=1.42</code> using:</p><pre><code class="language-julia">result = optimize(cost_function, [1.42], BFGS())</code></pre><p>Note that some of the algorithms may be sensitive to the initial condition. For more details on using Optim.jl, see the <a href="https://julianlsolvers.github.io/Optim.jl/stable/">documentation for Optim.jl</a>. We can improve our solution by noting that the Lotka-Volterra equation requires that the parameters are positive. Thus <a href="https://julianlsolvers.github.io/Optim.jl/stable/#user/minimization/#box-constrained-optimization">following the Optim.jl documentation</a> we can add box constraints to ensure the optimizer only checks between 0.0 and 3.0 which improves the efficiency of our algorithm:</p><pre><code class="language-julia">lower = [0.0]
upper = [3.0]
result = optimize(cost_function, lower, upper, [1.42], Fminbox(BFGS()))</code></pre><p>Lastly, we can use the same tools to estimate multiple parameters simultaneously. Let&#39;s use the Lotka-Volterra equation with all parameters free:</p><pre><code class="language-julia">function f2(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]
end

u0 = [1.0;1.0]
tspan = (0.0,10.0)
p = [1.5,1.0,3.0,1.0]
prob = ODEProblem(f2,u0,tspan,p)</code></pre><p>We can build an objective function and solve the multiple parameter version just as before:</p><pre><code class="language-julia">cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data),
                                      maxiters=10000,verbose=false)
result_bfgs = Optim.optimize(cost_function, [1.3,0.8,2.8,1.2], Optim.BFGS())</code></pre><p>We can also use First-Differences in L2Loss by passing the kwarg <code>differ_weight</code> which decides the contribution of the differencing loss to the total loss.</p><pre><code class="language-julia">cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data,differ_weight=0.3,data_weight=0.7),
                                      maxiters=10000,verbose=false)
result_bfgs = Optim.optimize(cost_function, [1.3,0.8,2.8,1.2], Optim.BFGS())</code></pre><p>To solve it using LeastSquaresOptim.jl, we use the <code>build_lsoptim_objective</code> function:</p><pre><code class="language-julia">cost_function = build_lsoptim_objective(prob1,t,data,Tsit5())</code></pre><p>The result is a cost function which can be used with LeastSquaresOptim. For more details, consult the <a href="https://github.com/matthieugomez/LeastSquaresOptim.jl">documentation for LeastSquaresOptim.jl</a>:</p><pre><code class="language-julia">x = [1.3,0.8,2.8,1.2]
res = optimize!(LeastSquaresProblem(x = x, f! = cost_function,
                output_length = length(t)*length(prob.u0)),
                LeastSquaresOptim.Dogleg(),LeastSquaresOptim.LSMR())</code></pre><p>We can see the results are:</p><pre><code class="language-julia">println(res.minimizer)

Results of Optimization Algorithm
 * Algorithm: Dogleg
 * Minimizer: [1.4995074428834114,0.9996531871795851,3.001556360700904,1.0006272074128821]
 * Sum of squares at Minimum: 0.035730
 * Iterations: 63
 * Convergence: true
 * |x - x&#39;| &lt; 1.0e-15: true
 * |f(x) - f(x&#39;)| / |f(x)| &lt; 1.0e-14: false
 * |g(x)| &lt; 1.0e-14: false
 * Function Calls: 64
 * Gradient Calls: 9
 * Multiplication Calls: 135</code></pre><p>and thus this algorithm was able to correctly identify all four parameters.</p><p>We can also use Multiple Shooting method by creating a <code>multiple_shooting_objective</code></p><pre><code class="language-julia">function ms_f(du,u,p,t)
  dx = p[1]*u[1] - p[2]*u[1]*u[2]
  dy = -3*u[2] + u[1]*u[2]
end
ms_u0 = [1.0;1.0]
tspan = (0.0,10.0)
ms_p = [1.5,1.0]
ms_prob = ODEProblem(ms_f,ms_u0,tspan,ms_p)
t = collect(range(0,stop=10,length=200))
data = Array(solve(ms_prob,Tsit5(),saveat=t,abstol=1e-12,reltol=1e-12))
bound = Tuple{Float64, Float64}[(0, 10),(0, 10),(0, 10),(0, 10),
                                (0, 10),(0, 10),(0, 10),(0, 10),
                                (0, 10),(0, 10),(0, 10),(0, 10),
                                (0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10)]


ms_obj = multiple_shooting_objective(ms_prob,Tsit5(),L2Loss(t,data);discontinuity_weight=1.0,abstol=1e-12,reltol=1e-12)</code></pre><p>This creates the objective function that can be passed to an optimizer from which we can then get the parameter values and the initial values of the short time periods keeping in mind the indexing.</p><pre><code class="language-julia">result = bboptimize(ms_obj;SearchRange = bound, MaxSteps = 21e3)
result.archive_output.best_candidate[end-1:end]</code></pre><p>Giving us the results as</p><pre><code class="language-julia">Starting optimization with optimizer BlackBoxOptim.DiffEvoOpt{BlackBoxOptim.FitPopulation{Float64},BlackBoxOptim.RadiusLimitedSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},BlackBoxOptim.RandomBound{BlackBoxOptim.RangePerDimSearchSpace}}

Optimization stopped after 21001 steps and 136.60030698776245 seconds
Termination reason: Max number of steps (21000) reached
Steps per second = 153.7405036862868
Function evals per second = 154.43596332393247
Improvements/step = 0.17552380952380953
Total function evaluations = 21096


Best candidate found: [0.997396, 1.04664, 3.77834, 0.275823, 2.14966, 4.33106, 1.43777, 0.468442, 6.22221, 0.673358, 0.970036, 2.05182, 2.4216, 0.274394, 5.64131, 3.38132, 1.52826, 1.01721]

Fitness: 0.126884213

Out[4]:2-element Array{Float64,1}:
        1.52826
        1.01721</code></pre><p>Here as our model had 2 parameters, we look at the last two indexes of <code>result</code> to get our parameter values and the rest of the values are the initial values of the shorter timespans as described in the reference section.</p><p>The objective function for Two Stage method can be created and passed to an optimizer as</p><pre><code class="language-julia">two_stage_obj = two_stage_method(ms_prob,t,data)
result = Optim.optimize(two_stage_obj, [1.3,0.8,2.8,1.2], Optim.BFGS()
)
Results of Optimization Algorithm
 * Algorithm: BFGS
 * Starting Point: [1.3,0.8,2.8,1.2]
 * Minimizer: [1.5035938533664717,0.9925731153746833, ...]
 * Minimum: 1.513400e+00
 * Iterations: 9
 * Convergence: true
   * |x - x&#39;| ≤ 0.0e+00: false
     |x - x&#39;| = 4.58e-10
   * |f(x) - f(x&#39;)| ≤ 0.0e+00 |f(x)|: false
     |f(x) - f(x&#39;)| = 5.87e-16 |f(x)|
   * |g(x)| ≤ 1.0e-08: true
     |g(x)| = 7.32e-11
   * Stopped by an increasing objective: false
   * Reached Maximum Number of Iterations: false
 * Objective Calls: 31
 * Gradient Calls: 31</code></pre><p>The default kernel used in the method is <code>Epanechnikov</code> others that are available are <code>Uniform</code>,  <code>Triangular</code>, <code>Quartic</code>, <code>Triweight</code>, <code>Tricube</code>, <code>Gaussian</code>, <code>Cosine</code>, <code>Logistic</code> and <code>Sigmoid</code>, this can be passed by the <code>kernel</code> keyword argument. <code>loss_func</code> keyword argument can be used to pass the loss function (cost function) you want  to use and <code>mpg_autodiff</code> enables Auto Differentiation.</p><h2 id="More-Algorithms-(Global-Optimization)-via-MathProgBase-Solvers"><a class="docs-heading-anchor" href="#More-Algorithms-(Global-Optimization)-via-MathProgBase-Solvers">More Algorithms (Global Optimization) via MathProgBase Solvers</a><a id="More-Algorithms-(Global-Optimization)-via-MathProgBase-Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#More-Algorithms-(Global-Optimization)-via-MathProgBase-Solvers" title="Permalink"></a></h2><p>The <code>build_loss_objective</code> function builds an objective function which is able to be used with MathProgBase-associated solvers. This includes packages like IPOPT, NLopt, MOSEK, etc. Building off of the previous example, we can build a cost function for the single parameter optimization problem like:</p><pre><code class="language-julia">function f(du,u,p,t)
  dx = p[1]*u[1] - u[1]*u[2]
  dy = -3*u[2] + u[1]*u[2]
end

u0 = [1.0;1.0]
tspan = (0.0,10.0)
p = [1.5]
prob = ODEProblem(f,u0,tspan,p)
sol = solve(prob,Tsit5())

t = collect(range(0,stop=10,length=200))
randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])
data = convert(Array,randomized)

obj = build_loss_objective(prob,Tsit5(),L2Loss(t,data),maxiters=10000)</code></pre><p>We can now use this <code>obj</code> as the objective function with MathProgBase solvers. For our example, we will use NLopt. To use the local derivative-free Constrained Optimization BY Linear Approximations algorithm, we can simply do:</p><pre><code class="language-julia">using NLopt
opt = Opt(:LN_COBYLA, 1)
min_objective!(opt, obj)
(minf,minx,ret) = NLopt.optimize(opt,[1.3])</code></pre><p>This finds a minimum at <code>[1.49997]</code>. For a modified evolutionary algorithm, we can use:</p><pre><code class="language-julia">opt = Opt(:GN_ESCH, 1)
min_objective!(opt, obj)
lower_bounds!(opt,[0.0])
upper_bounds!(opt,[5.0])
xtol_rel!(opt,1e-3)
maxeval!(opt, 100000)
(minf,minx,ret) = NLopt.optimize(opt,[1.3])</code></pre><p>We can even use things like the Improved Stochastic Ranking Evolution Strategy (and add constraints if needed). This is done via:</p><pre><code class="language-julia">opt = Opt(:GN_ISRES, 1)
min_objective!(opt, obj.cost_function2)
lower_bounds!(opt,[-1.0])
upper_bounds!(opt,[5.0])
xtol_rel!(opt,1e-3)
maxeval!(opt, 100000)
(minf,minx,ret) = NLopt.optimize(opt,[0.2])</code></pre><p>which is very robust to the initial condition. The fastest result comes from the following:</p><pre><code class="language-julia">using NLopt
opt = Opt(:LN_BOBYQA, 1)
min_objective!(opt, obj)
(minf,minx,ret) = NLopt.optimize(opt,[1.3])</code></pre><p>For more information, see the NLopt documentation for more details. And give IPOPT or MOSEK a try!</p><h2 id="Using-JuMP-with-DiffEqParamEstim"><a class="docs-heading-anchor" href="#Using-JuMP-with-DiffEqParamEstim">Using JuMP with DiffEqParamEstim</a><a id="Using-JuMP-with-DiffEqParamEstim-1"></a><a class="docs-heading-anchor-permalink" href="#Using-JuMP-with-DiffEqParamEstim" title="Permalink"></a></h2><p><a href="https://github.com/JuliaOpt/JuMP.jl">JuMP</a> is a domain-specific modeling language for mathematical optimization embedded in Julia.</p><pre><code class="language-julia">using OrdinaryDiffEq, DiffEqParamEstim, JuMP, NLopt, Plots</code></pre><p>Let&#39;s define the Lorenz equation to use as our example</p><pre><code class="language-julia">function g(du,u,p,t)
  σ,ρ,β = p
  x,y,z = u
  du[1] = dx = σ*(y-x)
  du[2] = dy = x*(ρ-z) - y
  du[3] = dz = x*y - β*z
end</code></pre><p>Let&#39;s get a solution of the system with parameter values <code>σ=10.0</code> <code>ρ=28.0</code> <code>β=8/3</code> to use as our data. We define some convenience functions <code>model_ode</code> (to create an <code>ODEProblem</code>) and <code>solve_model</code>(to obtain solution of the <code>ODEProblem</code>) to use in a custom objective function later.</p><pre><code class="language-julia">u0 = [1.0;0.0;0.0]
t = 0.0:0.01:1.0
tspan = (0.0,1.0)
model_ode(p_) = ODEProblem(g, u0, tspan,p_)
solve_model(mp_) = OrdinaryDiffEq.solve(model_ode(mp_), Tsit5(),saveat=0.01)
mock_data = Array(solve_model([10.0,28.0,8/3]))</code></pre><p>Now we define a custom objective function to pass for optimization to JuMP using the <code>build_loss_objective</code> described above provided by DiffEqParamEstim that defines an objective function for the parameter estimation problem.</p><pre><code class="language-julia">loss_objective(mp_, dat) = build_loss_objective(model_ode(mp_), Tsit5(), L2Loss(t,dat))</code></pre><p>We create a JuMP model, variables, set the objective function and the choice of optimization algorithm to be used in the JuMP syntax. You can read more about this in JuMP&#39;s <a href="http://www.juliaopt.org/JuMP.jl/0.18/index.html">documentation</a>.</p><pre><code class="language-julia">juobj(args...) = loss_objective(args, mock_data)(args)
jumodel = Model()
JuMP.register(jumodel, :juobj, 3, juobj, autodiff=true)
@variables jumodel begin
    σ,(start=8)
    ρ,(start=25.0)
    β,(start=10/3)
end
@NLobjective(jumodel, Min, juobj(σ, ρ, β))
setsolver(jumodel, NLoptSolver(algorithm=:LD_MMA))</code></pre><p>Let&#39;s call the optimizer to obtain the fitted parameter values.</p><pre><code class="language-julia">sol = JuMP.solve(jumodel)
best_mp = getvalue.(getindex.((jumodel,), Symbol.(jumodel.colNames)))</code></pre><p>Let&#39;s compare the solution at the obtained parameter values and our data.</p><pre><code class="language-julia">sol = OrdinaryDiffEq.solve(best_mp |&gt; model_ode, Tsit5())
plot(getindex.(sol.(t),1))
scatter!(mock_data, markersize=2)</code></pre><p><img src="assets/jumpestimationplot.png" alt="jumpestimationplot"/></p><h2 id="Generalized-Likelihood-Example"><a class="docs-heading-anchor" href="#Generalized-Likelihood-Example">Generalized Likelihood Example</a><a id="Generalized-Likelihood-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Generalized-Likelihood-Example" title="Permalink"></a></h2><p>In this example we will demo the likelihood-based approach to parameter fitting. First let&#39;s generate a dataset to fit. We will re-use the Lotka-Volterra equation but in this case fit just two parameters.</p><pre><code class="language-julia">f1 = function (du,u,p,t)
  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]
  du[2] = -3.0 * u[2] + u[1]*u[2]
end
p = [1.5,1.0]
u0 = [1.0;1.0]
tspan = (0.0,10.0)
prob1 = ODEProblem(f1,u0,tspan,p)
sol = solve(prob1,Tsit5())</code></pre><p>This is a function with two parameters, <code>[1.5,1.0]</code> which generates the same ODE solution as before. This time, let&#39;s generate 100 datasets where at each point adds a little bit of randomness:</p><pre><code class="language-julia">using RecursiveArrayTools # for VectorOfArray
t = collect(range(0,stop=10,length=200))
function generate_data(sol,t)
  randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])
  data = convert(Array,randomized)
end
aggregate_data = convert(Array,VectorOfArray([generate_data(sol,t) for i in 1:100]))</code></pre><p>here with <code>t</code> we measure the solution at 200 evenly spaced points. Thus <code>aggregate_data</code> is a 2x200x100 matrix where <code>aggregate_data[i,j,k]</code> is the <code>i</code>th component at time <code>j</code> of the <code>k</code>th dataset. What we first want to do is get a matrix of distributions where <code>distributions[i,j]</code> is the likelihood of component <code>i</code> at take <code>j</code>. We can do this via <code>fit_mle</code> on a chosen distributional form. For simplicity we choose the <code>Normal</code> distribution. <code>aggregate_data[i,j,:]</code> is the array of points at the given component and time, and thus we find the distribution parameters which fits best at each time point via:</p><pre><code class="language-julia">using Distributions
distributions = [fit_mle(Normal,aggregate_data[i,j,:]) for i in 1:2, j in 1:200]</code></pre><p>Notice for example that we have:</p><pre><code class="language-julia">julia&gt; distributions[1,1]
Distributions.Normal{Float64}(μ=1.0022440583676806, σ=0.009851964521952437)</code></pre><p>that is, it fit the distribution to have its mean just about where our original solution was and the variance is about how much noise we added to the dataset. This this is a good check to see that the distributions we are trying to fit our parameters to makes sense.</p><p>Note that in this case the <code>Normal</code> distribution was a good choice, and in many cases it&#39;s a nice go-to choice, but one should experiment with other choices of distributions as well. For example, a <code>TDist</code> can be an interesting way to incorporate robustness to outliers since low degrees of free T-distributions act like Normal distributions but with longer tails (though <code>fit_mle</code> does not work with a T-distribution, you can get the means/variances and build appropriate distribution objects yourself).</p><p>Once we have the matrix of distributions, we can build the objective function corresponding to that distribution fit:</p><pre><code class="language-julia">using DiffEqParamEstim
obj = build_loss_objective(prob1,Tsit5(),LogLikeLoss(t,distributions),
                                     maxiters=10000,verbose=false)</code></pre><p>First let&#39;s use the objective function to plot the likelihood landscape:</p><pre><code class="language-julia">using Plots; plotly()
prange = 0.5:0.1:5.0
heatmap(prange,prange,[obj([j,i]) for i in prange, j in prange],
        yscale=:log10,xlabel=&quot;Parameter 1&quot;,ylabel=&quot;Parameter 2&quot;,
        title=&quot;Likelihood Landscape&quot;)</code></pre><p><img src="assets/2paramlike.png" alt="2 Parameter Likelihood"/></p><p>Recall that this is the negative loglikelihood and thus the minimum is the maximum of the likelihood. There is a clear valley where the first parameter is 1.5, while the second parameter&#39;s likelihood is more muddled. By taking a one-dimensional slice:</p><pre><code class="language-julia">plot(prange,[obj([1.5,i]) for i in prange],lw=3,
     title=&quot;Parameter 2 Likelihood (Parameter 1 = 1.5)&quot;,
     xlabel = &quot;Parameter 2&quot;, ylabel = &quot;Objective Function Value&quot;)</code></pre><p><img src="assets/1paramlike.png" alt="1 Parameter Likelihood"/></p><p>we can see that there&#39;s still a clear minimum at the true value. Thus we will use the global optimizers from BlackBoxOptim.jl to find the values. We set our search range to be from <code>0.5</code> to <code>5.0</code> for both of the parameters and let it optimize:</p><pre><code class="language-julia">using BlackBoxOptim
bound1 = Tuple{Float64, Float64}[(0.5, 5),(0.5, 5)]
result = bboptimize(obj;SearchRange = bound1, MaxSteps = 11e3)

Starting optimization with optimizer BlackBoxOptim.DiffEvoOpt{BlackBoxOptim.FitPopulation{Float64},B
lackBoxOptim.RadiusLimitedSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},BlackBoxOptim.RandomBound
{BlackBoxOptim.RangePerDimSearchSpace}}
0.00 secs, 0 evals, 0 steps
0.50 secs, 1972 evals, 1865 steps, improv/step: 0.266 (last = 0.2665), fitness=-737.311433781
1.00 secs, 3859 evals, 3753 steps, improv/step: 0.279 (last = 0.2913), fitness=-739.658421879
1.50 secs, 5904 evals, 5799 steps, improv/step: 0.280 (last = 0.2830), fitness=-739.658433715
2.00 secs, 7916 evals, 7811 steps, improv/step: 0.225 (last = 0.0646), fitness=-739.658433715
2.50 secs, 9966 evals, 9861 steps, improv/step: 0.183 (last = 0.0220), fitness=-739.658433715

Optimization stopped after 11001 steps and 2.7839999198913574 seconds
Termination reason: Max number of steps (11000) reached
Steps per second = 3951.50873439296
Function evals per second = 3989.2242527195904
Improvements/step = 0.165
Total function evaluations = 11106


Best candidate found: [1.50001, 1.00001]

Fitness: -739.658433715</code></pre><p>This shows that it found the true parameters as the best fit to the likelihood.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../methods/optimization_based_methods/">« Optimization-Based Methods</a><a class="docs-footer-nextpage" href="../../tutorials/stochastic_evaluations/">Parameter Estimation for Stochastic Differential Equations and Ensembles »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 15 November 2020 16:15">Sunday 15 November 2020</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
